{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "CartPole is pretty much the Hello World problem for Reinforcement learning. In this Excercies we will first explore the observation/action space of the environment and then create a Neural Network capable of solving it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym\n",
    "import gym, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# to display plots in our notebook\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Environment\n",
    "Openai was kind enough to create a now commonly used collection of reinforcement learning problems with an easy to use API, called openai gym (https://gym.openai.com/envs/#classic_control). Whilst this extensive library includes all kinds of interesting environments such as Atari games or robotic control, today we will focus on the \"hello world\" problem of RL, namely CartPole.\n",
    "Before we can start building our Neural Network, we should first try to understand the problem at hand. To that end, we will execute the environment with random actions whilst rendering each frame and printing everything we can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")     # initialize the environment \n",
    "\n",
    "\n",
    "# before executing the environment, we have to reset it \n",
    "# (which will return the initial observation)\n",
    "for _ in range(10):\n",
    "    observation = env.reset()\n",
    "    done = False \n",
    "    while not done:\n",
    "        action = env.action_space.sample()   # a sample action from the allowed action-space\n",
    "        env.render() # not necessary and not advisible whilst training (slows everything down substatially)\n",
    "        time.sleep(1/45)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        print(f\"Action:\\t\\t{action}\")\n",
    "        print(f\"Observation:\\t{observation}\")\n",
    "        print(f\"Reward:\\t\\t{reward}\")\n",
    "        print(f\"Done:\\t\\t{done}\")\n",
    "        print(f\"Info:\\t\\t{info}\\n\")\n",
    "env.close() # not 100% necessary but good practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's analyse what we can see above.\n",
    "\n",
    "    1. Action\n",
    "        As expected, the action-space is discrete and binary (either 0 or 1).\n",
    "        A quick check of the documentation (https://github.com/openai/gym/wiki/CartPole-v0)\n",
    "        reveals that action 0 represents a \"push\" left, and action 1 a \"push\" to the right.\n",
    "\n",
    "    2. Observation\n",
    "        The observation space consists of 4 floats, namely:\n",
    "        [0]   Cart Position   [-2.4,2.4]\n",
    "        [1]   Cart Velocity   [-Inf,Inf]\n",
    "        [2]   Pole Angle      [-41.8,41.8]\n",
    "        [3]   Pole Velocity   [-Inf,Inf]\n",
    "\n",
    "    3. Reward\n",
    "        The reward function for this environment is very straight forward. \n",
    "        For every frame you survive it will return 1. Therefore total reward = # frames survived\n",
    "        \n",
    "    4. Done\n",
    "        Three different conditions can terminate the game (set done to True)\n",
    "            1. The absolute Pole Angle is greater than 12\n",
    "            2. The absolute Car Position is greater than 2.4\n",
    "            3. After 200 frames (solved)\n",
    "\n",
    "    5. Info\n",
    "        In this environment \"Info\" is empty. In atari environments \n",
    "        (https://gym.openai.com/envs/#atari) this will represent the lives.\n",
    "        \n",
    "        \n",
    "        \n",
    "The official openai documentation considers this environment solved when the average reward over 100 episodes is greater/equal 195.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we now have a pretty good understanding of the environment, the question remains, how can we solve it.\n",
    "\n",
    "# DQN (Deep Q Learning)\n",
    "The usual mainstream approach would be to use a technique called Deep Q Learning, which was first introduced in DeepMind's legendary paper \"Human-level control through deep reinforcement learning\" (link: https://deepmind.com/research/open-source/dqn). The Q in DQN stands for Quality, which already gives away what this strategy is all about. Namely, estimating the Quality of all available actions during a specific observation (i.e. at the current state of the game, if I were to choose action 1, what do I estimate the total score to be. In this tutorial, however, we will take a slightly more primitive approach, namely, turn the task at hand into a classification problem.\n",
    "\n",
    "# Epsilon-Decreasing\n",
    "Since we decided to approach this challenge as a Binary Classification (action_space = 2), we need to have a function that determines whether a state-action pair is appended to the training data. One straight forward approach to doing this is by only appending the data if the current score exceeds some threshold (which we will increase over time).\n",
    "\n",
    "Now, to make sure that we do not become biased towards a specific action to soo (exploitation), we will continue to introduce some random actions (exploration). This is done via a variable called epsilon (representing the percentage probability of taking a random action), which we will gradually decrease over time.\n",
    "\n",
    "One of the challenges here will be to pick a reasonable score-threshold. So let's play a few games and keep track of the score so that we get a basic understanding of how well a random agent will perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_list = []\n",
    "\n",
    "for game_nr in range(1_000):\n",
    "    score = done = 0   # in python False==0\n",
    "    env.reset()\n",
    "    while not done:\n",
    "        _, reward, done, _ = env.step(env.action_space.sample())\n",
    "        score+=reward\n",
    "    score_list.append(score)\n",
    "    print(f\"{game_nr}/{100}\", end=\"\\r\") # to keep track of things (not necessary)\n",
    "env.close() # not necessary but good practice\n",
    "\n",
    "print(f\"Min Score:     {np.min(score_list)}\")\n",
    "print(f\"Mean Score:    {np.mean(score_list)}\")\n",
    "print(f\"Median Score:  {np.median(score_list)}\")\n",
    "print(f\"Max Score:     {np.max(score_list)}\")\n",
    "\n",
    "plt.hist(score_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like something around __ would sever as a good initial threshold. To be honest, this choice is somewhat arbitrary and I would encourage you to play around with it a little."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "Now that we know what our initial score-threshold will be, let's build the Neural Network. Since this is something that has already been covered, I'll purpousefully create an architecture that will not converge, so that you can play around with it a little."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(env.observation_space.sample()) # number of features \n",
    "num_labels = 1 #env.action_space.n   we use 1 instead of 2 since this is binary\n",
    "\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(n, \"hidden-layer-1\"),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(\"hidden-layer-1\", \"hidden-layer-2\"),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(\"hidden-layer-2\", num_labels),\n",
    "    nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(obs):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # exploration\n",
    "        return env.action_space.sample()\n",
    "    \n",
    "    else:\n",
    "        # exploitation\n",
    "        obs = Variable(torch.from_numpy(obs.reshape(-1)).float(), requires_grad=False)\n",
    "        return int(net(obs).detach().numpy() > .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the network and all relevant variables/CONSTANTS\n",
    "LR = 1e-2\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "loss_fn = nn.BCELoss() # Binary Cross-Entropy loss (since this is a binary classification problem)\n",
    "optimizer = optim.Adam(net.parameters(), lr=LR)\n",
    "\n",
    "\n",
    "epsilon = 1. #100% exploration (initially)\n",
    "EPSILON_DECAY = \"This is for you to play around with\"\n",
    "\n",
    "score_threshold = \"pretty arbitrary tbh\"\n",
    "\n",
    "TOTAL_GAMES = 20_000\n",
    "X_train = []\n",
    "y_train = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot our epsilon decay as a quick sanity check\n",
    "\n",
    "# the first plot is very straight forward, we simply compare the epsilon to each training loop\n",
    "plt.plot(np.arange(0, 50), epsilon*EPSILON_DECAY**(np.arange(0, 50)))\n",
    "plt.title(\"Epsilon-Decay - Training\")\n",
    "plt.xlabel(\"Training-loops\")\n",
    "plt.ylabel(\"epsilon\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is set up, we are ready to start our training process. Recall that for this environment to be solved, you will have to achieve an average score of 195 (or above) for 100 consecutive episodes. The fewer training episodes you require to achieve this, the better. We will check after each training whether we are able to solve it (using 100% exploitation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_solved():\n",
    "    avg_score = 0\n",
    "    for c_game in range(1, 101):\n",
    "        done = 0\n",
    "        obs = env.reset()\n",
    "        while not done:\n",
    "            obs = Variable(torch.from_numpy(obs.reshape(-1)).float(), requires_grad=False)\n",
    "            action = int(net(obs).detach().numpy() > .5)\n",
    "            \n",
    "            obs,reward,done,_ = env.step(action)\n",
    "            env.render() # optional\n",
    "            #time.sleep(1/100)\n",
    "            avg_score += reward\n",
    "            \n",
    "        # check for early exit condition\n",
    "        if (avg_score+200*(100-c_game))/100 < 195:\n",
    "            return False, avg_score/c_game\n",
    "            \n",
    "            \n",
    "    return avg_score/100 >= 195.0, avg_score/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for game_nr in range(TOTAL_GAMES):\n",
    "    score = done = 0  # in python 0 == False\n",
    "    obs = env.reset()\n",
    "    \n",
    "    # game_obs & game_actions are used to keep track of what happened\n",
    "    # during the game before we decide whether we will use the data\n",
    "    game_obs = []\n",
    "    game_action = []\n",
    "    \n",
    "    while not done:\n",
    "        action = get_action(obs)\n",
    "        \n",
    "        # keep track of local state-action pairs\n",
    "        game_obs.append(obs)\n",
    "        game_action.append(action)\n",
    "        \n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        score+=reward\n",
    "        \n",
    "    print(f\"{game_nr}/{TOTAL_GAMES}\\tscore: {score:.2f}\", end=\"\\r\")\n",
    "    score_list.append(score)\n",
    "    \n",
    "    # check whether the game was good enough\n",
    "    if score >= score_threshold:\n",
    "        X_train += game_obs      # Thank you Guido van Rossum for\n",
    "        y_train += game_action   # making this possible\n",
    "        \n",
    "    # check whether we should re-train the NN\n",
    "    if len(X_train) > BATCH_SIZE:\n",
    "        # first, let's print stuff to keep track of what is going on\n",
    "        print(f\"{game_nr} / {TOTAL_GAMES}\"+\\\n",
    "              f\"\\tAverage Score: {np.mean(score):.2f}\"+\\\n",
    "              f\"\\tScore Threshold: {score_threshold:.2f}\"+\\\n",
    "              f\"\\tEpsilon: {epsilon:.2f}\")\n",
    "        \n",
    "        X_train = np.asarray(X_train)\n",
    "        y_train = np.asarray(y_train)\n",
    "        \n",
    "        # train the network\n",
    "        for epoch in range(EPOCHS):\n",
    "            # shuffle the data\n",
    "            indices = np.arange(X_train.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "\n",
    "            X_train = X_train[indices]\n",
    "            y_train = y_train[indices]\n",
    "            \n",
    "            avg_loss = 0 # to keep track of our epoch wise loss\n",
    "            \n",
    "            \n",
    "            for i, (X, y) in enumerate(zip(X_train, y_train)):\n",
    "                # Since this is not keras, we will have to print our own progress bar\n",
    "                if not (i%25):\n",
    "                    pct = i/len(y_train)\n",
    "                    print(f\"{epoch}/{EPOCHS}\"+\\\n",
    "                          f\"|\"+\"#\"*int(pct*25)+\\\n",
    "                          f\" \"*(25-int(pct*25))+\"|\"+\\\n",
    "                          f\" {pct*100:.2f}%\", end=\"\\r\")\n",
    "                \n",
    "                X = Variable(torch.from_numpy(X).float(), requires_grad=False)\n",
    "                net.zero_grad()\n",
    "                y_pred = net(X)\n",
    "                loss = loss_fn(y_pred, torch.from_numpy(y.reshape(1).astype(\"float32\")))\n",
    "                avg_loss += (1/len(y_train)*loss.item())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            print(f\"{epoch}/{EPOCHS}\"+\\\n",
    "                  f\"\\tEpoch-wise average loss: {avg_loss:.5f}             \")\n",
    "        \n",
    "        solved, avg_score = check_if_solved()\n",
    "        if solved:\n",
    "            print(f\"Environment successfully solved with an average score of \"+\\\n",
    "                  f\"{avg_score} after {game_nr} episodes\")\n",
    "            break\n",
    "        # reset our training data\n",
    "        X_train = []\n",
    "        y_train = []\n",
    "        \n",
    "        # adding some scalar is a very primitive approach, you should try to \n",
    "        # to use something like np.percentil or np.mean\n",
    "        score_threshold += 15  \n",
    "        \n",
    "        # decay the epsilon (this is something you might want to adjust)\n",
    "        epsilon *= EPSILON_DECAY\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
